{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dc5237d-4a0b-4615-bd08-89e5cafb36d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data\n",
    "import os\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "original_dataset_dir = \"../Project4/images/Images\"\n",
    "base_dir = \"../Project4/split_data\"\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "#Validation set for overfitting of complex models\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "# Ensure directories exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)\n",
    "\n",
    "# Define classes\n",
    "classes = os.listdir(original_dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b43a6ae-e9df-4e41-8299-c09a69cfb3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for each class in train and test directories\n",
    "for class_name in classes:\n",
    "    class_train_dir = os.path.join(train_dir, class_name)\n",
    "    class_test_dir = os.path.join(test_dir, class_name)\n",
    "    class_val_dir = os.path.join(val_dir, class_name)\n",
    "    os.makedirs(class_train_dir, exist_ok=True)\n",
    "    os.makedirs(class_test_dir, exist_ok=True)\n",
    "    os.makedirs(class_val_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5665cd9d-18f1-43c8-9bd3-bc7d51421811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split and directories created successfully.\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "split_ratio_train = 0.7  # 70% train\n",
    "split_ratio_val = 0.15  # 15% validation (from train data)\n",
    "split_ratio_test = 0.15  # 15% test\n",
    "\n",
    "for class_name in classes:\n",
    "  class_images = os.listdir(os.path.join(original_dataset_dir, class_name))\n",
    "  shuffle(class_images)  # Randomize image order\n",
    "\n",
    "  num_images = len(class_images)\n",
    "\n",
    "  # Calculate split points (adjust ratios as needed)\n",
    "  num_train_images = int(split_ratio_train * num_images)\n",
    "  num_val_images = int(split_ratio_val * num_images)\n",
    "  num_test_images = num_images - num_train_images - num_val_images\n",
    "\n",
    "  # Train set\n",
    "  for image_name in class_images[:num_train_images]:\n",
    "    src = os.path.join(original_dataset_dir, class_name, image_name)\n",
    "    dst = os.path.join(train_dir, class_name, image_name)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "  # Validation set (from train data)\n",
    "  for image_name in class_images[num_train_images:num_train_images+num_val_images]:\n",
    "    src = os.path.join(original_dataset_dir, class_name, image_name)\n",
    "    dst = os.path.join(val_dir, class_name, image_name)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "  # Test set\n",
    "  for image_name in class_images[num_train_images+num_val_images:]:\n",
    "    src = os.path.join(original_dataset_dir, class_name, image_name)\n",
    "    dst = os.path.join(test_dir, class_name, image_name)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "print(\"Data split and directories created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1fdd3cb-dbe2-45e6-acb8-3fa7eb46366d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images: 20580\n",
      "Total Train Images: 14355, Percent of Total: 69.75218658892129\n",
      "Total Test Images: 3200, Percent of Total: 15.54907677356657\n",
      "Total Validation Images: 3025, Percent of Total: 14.698736637512146\n",
      "Split is Effective!\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "#Check the splits worked\n",
    "# Count the total number of images in train and test directories\n",
    "#Using underscores to save memory by not storing labels,path, or directories and simply count number of files in each sub directory\n",
    "total_train_count = sum(len(files) for _, _, files in os.walk(train_dir))\n",
    "total_test_count = sum(len(files) for _, _, files in os.walk(test_dir))\n",
    "total_val_count = sum(len(files) for _, _, files in os.walk(val_dir))\n",
    "total_images = total_test_count+total_train_count+total_val_count\n",
    "train_per = total_train_count/total_images *100\n",
    "test_per = total_test_count/total_images*100\n",
    "val_per = total_val_count/total_images *100\n",
    "\n",
    "print(f\"Total Images: {total_images}\")\n",
    "print(f\"Total Train Images: {total_train_count}, Percent of Total: {train_per}\")\n",
    "print(f\"Total Test Images: {total_test_count}, Percent of Total: {test_per}\")\n",
    "print(f\"Total Validation Images: {total_val_count}, Percent of Total: {val_per}\")\n",
    "print(\"Split is Effective!\")\n",
    "classes_val = os.listdir(val_dir)\n",
    "classes_test = os.listdir(test_dir)\n",
    "classes_train = os.listdir(train_dir)\n",
    "\n",
    "print(len(classes_val))\n",
    "print(len(classes_test))\n",
    "print(len(classes_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcbfc2d3-ac27-4951-9528-cb498386ddd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14355 images belonging to 120 classes.\n",
      "Found 3025 images belonging to 120 classes.\n"
     ]
    }
   ],
   "source": [
    "#Since we are no longer using simple sequential models and are Using the Functional API we rescale with generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,  # Rescale images to [0, 1]\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)  # Example augmentations\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)  # Rescale for validation\n",
    "\n",
    "# Create training and validation generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # Adjust image size as needed\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'  # Assuming categorical classification\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d57b765-cbf9-4b79-b9ed-dd5a9fea5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, Add, Input, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8da7b4ba-5e62-4c79-af39-cef3b62d7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now on to training our Models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35ebbf87-f2a9-4c46-bf08-4f9aa08760f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_block(x, filters):\n",
    "  # Shortcut connection\n",
    "  shortcut = x\n",
    "\n",
    "  # Convolutional layers with Batch Normalization\n",
    "  x = Conv2D(filters, kernel_size=(3, 3), strides=1, padding=\"same\")(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  x = Activation('relu')(x)  # ReLU activation\n",
    "\n",
    "  x = Conv2D(filters, kernel_size=(3, 3), strides=1, padding=\"same\")(x)\n",
    "  x = BatchNormalization()(x)\n",
    "\n",
    "  # Add shortcut connection with element-wise sum\n",
    "  x = Add()([x, shortcut])\n",
    "  x = Activation('relu')(x)  # ReLU activation after adding shortcut\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6527bbaf-3db5-4ace-9de3-cb6025d21aca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 112, 112, 64)         9472      ['input_4[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 112, 112, 64)         256       ['conv2d_14[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " activation_9 (Activation)   (None, 112, 112, 64)         0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPoolin  (None, 55, 55, 64)           0         ['activation_9[0][0]']        \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 55, 55, 64)           36928     ['max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 55, 55, 64)           256       ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " activation_10 (Activation)  (None, 55, 55, 64)           0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 55, 55, 64)           36928     ['activation_10[0][0]']       \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 55, 55, 64)           256       ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 55, 55, 64)           0         ['batch_normalization_11[0][0]\n",
      "                                                                    ',                            \n",
      "                                                                     'max_pooling2d_6[0][0]']     \n",
      "                                                                                                  \n",
      " activation_11 (Activation)  (None, 55, 55, 64)           0         ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3  (None, 64)                   0         ['activation_11[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " flatten_4 (Flatten)         (None, 64)                   0         ['global_average_pooling2d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 120)                  7800      ['flatten_4[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 91896 (358.97 KB)\n",
      "Trainable params: 91512 (357.47 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#ResNet Model\n",
    "inputs = Input(shape=(224, 224, 3))  # Input image shape\n",
    "\n",
    "#Initial convolutional layer 64 filters, 7x7\n",
    "x = Conv2D(64, kernel_size=(7, 7), strides=2, padding=\"same\")(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)  # ReLU activation\n",
    "\n",
    "#Max pooling\n",
    "x = MaxPooling2D(pool_size=(3, 3), strides=2)(x)\n",
    "\n",
    "#**Residual block 1**\n",
    "x = residual_block(x, 64)  # Replace 64 with desired number of filters\n",
    "\n",
    "#**Residual block 2**\n",
    "# You can add more residual blocks here (e.g., residual_block(x, 128))\n",
    "\n",
    "# Global average pooling\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "# Flatten and fully-connected layer\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(120, activation='softmax')(x)  # Change number of neurons for your classification task\n",
    "\n",
    "# Create the model using Model for more complex ResNet network\n",
    "ResNet = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "# Compile the model (adjust optimizer and loss as needed)\n",
    "ResNet.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "ResNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0461b9bc-75c5-4cdf-ac15-003e00c773a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "206/449 [============>.................] - ETA: 2:09 - loss: 4.8116 - accuracy: 0.0137"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = ResNet.fit(\n",
    "            train_generator,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545691c9-13ed-44b2-a7a9-d22aadc6ad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can directly import from Keras here\n",
    "from keras.applications.vgg16 import VGG16\n",
    "model_vgg16 = VGG16(weights='imagenet')\n",
    "model_vgg16.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e470b4d-d8d6-49d5-a829-b2204601e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model from image generator\n",
    "history = model_vgg16.fit(\n",
    "            train_generator,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56c7adec-2c07-4b6c-9b16-3991d9bd848f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 54, 54, 96)        34944     \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 26, 26, 96)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 26, 26, 256)       614656    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 256)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 12, 12, 384)       885120    \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 384)       1327488   \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)       884992    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 5, 5, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              26218496  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 120)               491640    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47238648 (180.20 MB)\n",
      "Trainable params: 47238648 (180.20 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model AlexNet\n",
    "AlexNet = models.Sequential()\n",
    "\n",
    "# Layer 1: Convolutional layer with 96 filters of size 11x11, followed by max pooling\n",
    "AlexNet.add(layers.Conv2D(96, kernel_size=(11,11),strides=4, activation='relu', input_shape=(224, 224, 3)))\n",
    "AlexNet.add(layers.MaxPooling2D(pool_size=(3, 3),strides=2))\n",
    "\n",
    "AlexNet.add(layers.Conv2D(256, kernel_size=(5,5),padding='same', activation='relu'))\n",
    "AlexNet.add(layers.MaxPooling2D(pool_size=(3, 3),strides=2))\n",
    "\n",
    "AlexNet.add(layers.Conv2D(384, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "AlexNet.add(layers.Conv2D(384, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "AlexNet.add(layers.Conv2D(256, kernel_size=(3,3),padding='same', activation='relu'))\n",
    "\n",
    "AlexNet.add(layers.MaxPooling2D(pool_size=(3, 3),strides=2))\n",
    "\n",
    "# Flatten the feature maps to feed into fully connected layers\n",
    "AlexNet.add(layers.Flatten())\n",
    "\n",
    "\n",
    "# Layer 3: Fully connected layer with 120 neurons\n",
    "AlexNet.add(layers.Dense(4096, activation='relu'))\n",
    "AlexNet.add(layers.Dropout(.5))\n",
    "# Layer 4: Fully connected layer with 84 neurons\n",
    "AlexNet.add(layers.Dense(4096, activation='relu'))\n",
    "AlexNet.add(layers.Dropout(.5))\n",
    "\n",
    "# Output layer: Fully connected layer with num_classes neurons (e.g., 3 )\n",
    "AlexNet.add(layers.Dense(120, activation='softmax'))\n",
    "\n",
    "# Compile model\n",
    "AlexNet.compile(optimizer=optimizers.RMSprop(learning_rate=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Generating the summary of the model\n",
    "AlexNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce36c6a-adc5-424f-8886-90b0e5974f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "411/411 [==============================] - 309s 749ms/step - loss: 4.7867 - accuracy: 0.0090 - val_loss: 4.7836 - val_accuracy: 0.0113\n",
      "Epoch 2/20\n",
      "411/411 [==============================] - 304s 740ms/step - loss: 4.7608 - accuracy: 0.0124 - val_loss: 4.6992 - val_accuracy: 0.0198\n",
      "Epoch 3/20\n",
      "411/411 [==============================] - 310s 754ms/step - loss: 4.6111 - accuracy: 0.0281 - val_loss: 4.6498 - val_accuracy: 0.0219\n",
      "Epoch 4/20\n",
      "411/411 [==============================] - 309s 750ms/step - loss: 4.4431 - accuracy: 0.0373 - val_loss: 4.3622 - val_accuracy: 0.0460\n",
      "Epoch 5/20\n",
      "411/411 [==============================] - 307s 747ms/step - loss: 4.3068 - accuracy: 0.0495 - val_loss: 4.2653 - val_accuracy: 0.0518\n",
      "Epoch 6/20\n",
      "411/411 [==============================] - 308s 748ms/step - loss: 4.1833 - accuracy: 0.0588 - val_loss: 4.2922 - val_accuracy: 0.0515\n",
      "Epoch 7/20\n",
      "411/411 [==============================] - 308s 749ms/step - loss: 4.0663 - accuracy: 0.0762 - val_loss: 4.1679 - val_accuracy: 0.0634\n",
      "Epoch 8/20\n",
      "411/411 [==============================] - 305s 741ms/step - loss: 3.9286 - accuracy: 0.0981 - val_loss: 4.1733 - val_accuracy: 0.0698\n",
      "Epoch 9/20\n",
      "411/411 [==============================] - 306s 744ms/step - loss: 3.7917 - accuracy: 0.1118 - val_loss: 3.8960 - val_accuracy: 0.0959\n",
      "Epoch 10/20\n",
      "411/411 [==============================] - 301s 731ms/step - loss: 3.6433 - accuracy: 0.1418 - val_loss: 3.8432 - val_accuracy: 0.1081\n",
      "Epoch 11/20\n",
      "411/411 [==============================] - 298s 724ms/step - loss: 3.4530 - accuracy: 0.1705 - val_loss: 3.8650 - val_accuracy: 0.1030\n",
      "Epoch 12/20\n",
      "411/411 [==============================] - 297s 722ms/step - loss: 3.2598 - accuracy: 0.2056 - val_loss: 3.8410 - val_accuracy: 0.1237\n",
      "Epoch 13/20\n",
      "411/411 [==============================] - 289s 703ms/step - loss: 3.0168 - accuracy: 0.2547 - val_loss: 4.1652 - val_accuracy: 0.1249\n",
      "Epoch 14/20\n",
      "411/411 [==============================] - 291s 709ms/step - loss: 2.7428 - accuracy: 0.3040 - val_loss: 4.0880 - val_accuracy: 0.1273\n",
      "Epoch 15/20\n",
      "411/411 [==============================] - 289s 704ms/step - loss: 2.4301 - accuracy: 0.3728 - val_loss: 3.9638 - val_accuracy: 0.1404\n",
      "Epoch 16/20\n",
      "411/411 [==============================] - 302s 735ms/step - loss: 2.0922 - accuracy: 0.4456 - val_loss: 4.3406 - val_accuracy: 0.1493\n",
      "Epoch 17/20\n",
      "411/411 [==============================] - 288s 699ms/step - loss: 1.7498 - accuracy: 0.5222 - val_loss: 4.7161 - val_accuracy: 0.1535\n",
      "Epoch 18/20\n",
      "411/411 [==============================] - 310s 754ms/step - loss: 1.4196 - accuracy: 0.6058 - val_loss: 5.0047 - val_accuracy: 0.1505\n",
      "Epoch 19/20\n",
      "411/411 [==============================] - 310s 753ms/step - loss: 1.1206 - accuracy: 0.6830 - val_loss: 5.4645 - val_accuracy: 0.1581\n",
      "Epoch 20/20\n",
      "411/411 [==============================] - 306s 743ms/step - loss: 0.8704 - accuracy: 0.7454 - val_loss: 5.9146 - val_accuracy: 0.1465\n"
     ]
    }
   ],
   "source": [
    "#fit the model from image generator\n",
    "history = AlexNet.fit(\n",
    "            train_rescale_ds,\n",
    "            batch_size=32,\n",
    "            epochs=20,\n",
    "            validation_data=val_rescale_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe00dfb4-0b2d-451a-b702-47fca334b5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can see major overfitting from epochs 11 on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
